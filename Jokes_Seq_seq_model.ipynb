{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jokes_Seq_seq_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQBOfFTz7Htg",
        "colab_type": "code",
        "outputId": "fdd08517-33c1-4dd4-cff9-c5ca15409e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-IN,en;q=0.9,en-US;q=0.8,te;q=0.7\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-data-sets/623/1190/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1588590612&Signature=rheWuPjPmNOLvUrlUIvvZX1pqg1ZKDfxYvKVeqxTH8qOYuTjeltFvDpJ7E2kpsGoNXsstY2%2F9i292rpSBHTd%2BHWLxW3ZLgHEOg62EqGrG15iwT1MdAPjcKFQcu0CzILrtclPlyHGsm4Z1Kb01HO%2FntP9qG6Nusi98PpfxG65tT8W%2FQPwASIIJc1mRL1GZVM91ZrfADBfxDQ13BY4qt%2F952a1sJkfJoDXUkRNiL3kPyx64sjaiRu3QC59DYG%2B%2BYYtHlM3Q1CJKbakOKJdiwcUzFLu75er6xQivhbaVkrhMCP6mONNRh6rM1sHxr%2FHWD1wf4eqbEfahLsuuG5Gj%2BM%2Fjw%3D%3D&response-content-disposition=attachment%3B+filename%3Dqa-jokes.zip\" -c -O 'qa-jokes.zip'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-01 12:31:40--  https://storage.googleapis.com/kaggle-data-sets/623/1190/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1588590612&Signature=rheWuPjPmNOLvUrlUIvvZX1pqg1ZKDfxYvKVeqxTH8qOYuTjeltFvDpJ7E2kpsGoNXsstY2%2F9i292rpSBHTd%2BHWLxW3ZLgHEOg62EqGrG15iwT1MdAPjcKFQcu0CzILrtclPlyHGsm4Z1Kb01HO%2FntP9qG6Nusi98PpfxG65tT8W%2FQPwASIIJc1mRL1GZVM91ZrfADBfxDQ13BY4qt%2F952a1sJkfJoDXUkRNiL3kPyx64sjaiRu3QC59DYG%2B%2BYYtHlM3Q1CJKbakOKJdiwcUzFLu75er6xQivhbaVkrhMCP6mONNRh6rM1sHxr%2FHWD1wf4eqbEfahLsuuG5Gj%2BM%2Fjw%3D%3D&response-content-disposition=attachment%3B+filename%3Dqa-jokes.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.13.80, 2607:f8b0:4004:811::2010\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.13.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1423390 (1.4M) [application/zip]\n",
            "Saving to: ‘qa-jokes.zip’\n",
            "\n",
            "\rqa-jokes.zip          0%[                    ]       0  --.-KB/s               \rqa-jokes.zip        100%[===================>]   1.36M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2020-05-01 12:31:40 (175 MB/s) - ‘qa-jokes.zip’ saved [1423390/1423390]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdFnITOm7msA",
        "colab_type": "code",
        "outputId": "12be54bc-d06b-42c3-b4cb-8a6b0f1594c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from zipfile import ZipFile \n",
        "  \n",
        "# specifying the zip file name \n",
        "file_name = \"qa-jokes.zip\"\n",
        "  \n",
        "# opening the zip file in READ mode \n",
        "with ZipFile(file_name, 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall() \n",
        "    print('Done!') \n",
        "del zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name                                             Modified             Size\n",
            "jokes.csv                                      2019-09-20 04:54:38      3508579\n",
            "Extracting all the files now...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etjLtVTg7wem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv('jokes.csv',nrows=15000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r_4KBnR71wh",
        "colab_type": "code",
        "outputId": "aaa54fdf-090b-4881-918e-2eadebda79d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Did you hear about the Native American man tha...</td>\n",
              "      <td>He nearly drown in his own tea pee.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>What's the best anti diarrheal prescription?</td>\n",
              "      <td>Mycheexarphlexin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>What do you call a person who is outside a doo...</td>\n",
              "      <td>Matt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Which Star Trek character is a member of the m...</td>\n",
              "      <td>Jean-Luc Pickacard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>What's the difference between a bullet and a h...</td>\n",
              "      <td>A bullet doesn't miss Harambe</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  ...                               Answer\n",
              "0   1  ...  He nearly drown in his own tea pee.\n",
              "1   2  ...                     Mycheexarphlexin\n",
              "2   3  ...                                 Matt\n",
              "3   4  ...                   Jean-Luc Pickacard\n",
              "4   5  ...        A bullet doesn't miss Harambe\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXUpqpzx72gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "import os\n",
        "import re\n",
        "path=os.getcwd()+'/jokes.csv'\n",
        "# To read the file\n",
        "import io\n",
        "import numpy as np\n",
        "# For normalizing data\n",
        "import unicodedata\n",
        "# Tensorflow libraries\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
        "from tensorflow.keras.layers import Input, Softmax, RNN, Dense, Embedding, LSTM,TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "# Metric that can tell the strength of relation between two sentences\n",
        "import nltk.translate.bleu_score as bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JMUpLex8He9",
        "colab_type": "code",
        "outputId": "c4f1067d-e4bf-49ca-e451-edd670715ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Did you hear about the Native American man tha...</td>\n",
              "      <td>He nearly drown in his own tea pee.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>What's the best anti diarrheal prescription?</td>\n",
              "      <td>Mycheexarphlexin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>What do you call a person who is outside a doo...</td>\n",
              "      <td>Matt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Which Star Trek character is a member of the m...</td>\n",
              "      <td>Jean-Luc Pickacard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>What's the difference between a bullet and a h...</td>\n",
              "      <td>A bullet doesn't miss Harambe</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  ...                               Answer\n",
              "0   1  ...  He nearly drown in his own tea pee.\n",
              "1   2  ...                     Mycheexarphlexin\n",
              "2   3  ...                                 Matt\n",
              "3   4  ...                   Jean-Luc Pickacard\n",
              "4   5  ...        A bullet doesn't miss Harambe\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2mObVcl8OrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting into unicode --http://stackoverflow.com/a/518232/2809427\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# Function to convert into lower case, remove punctuation.\n",
        "def normalize_string(s):\n",
        "    s = unicode_to_ascii(s.lower().strip())\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sPPy5tH8Omw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['question_preprocess']=data['Question'].apply(lambda x: normalize_string(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw3ew9Hl8OkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['answer_preprocess']=data['Answer'].apply(lambda x : normalize_string(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STIrOB2i8Oh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['answer_preprocess_in']=data['answer_preprocess'].apply(lambda x : '<start> '+x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po3B__0h8OgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['answer_preprocess_out']=data['answer_preprocess'].apply(lambda x: x+' <end>')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCK_R9z7DXJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['answer_preprocess']=data['Answer'].apply(lambda x : '<start> '+ x + ' <end>')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtcH8sop8OeC",
        "colab_type": "code",
        "outputId": "6f5d71a6-d525-4714-dc8b-7f001e4894f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>question_preprocess</th>\n",
              "      <th>answer_preprocess</th>\n",
              "      <th>answer_preprocess_in</th>\n",
              "      <th>answer_preprocess_out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Did you hear about the Native American man tha...</td>\n",
              "      <td>He nearly drown in his own tea pee.</td>\n",
              "      <td>did you hear about the native american man tha...</td>\n",
              "      <td>&lt;start&gt; He nearly drown in his own tea pee. &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; he nearly drown in his own tea pee .</td>\n",
              "      <td>he nearly drown in his own tea pee . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>What's the best anti diarrheal prescription?</td>\n",
              "      <td>Mycheexarphlexin</td>\n",
              "      <td>what s the best anti diarrheal prescription ?</td>\n",
              "      <td>&lt;start&gt; Mycheexarphlexin &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; mycheexarphlexin</td>\n",
              "      <td>mycheexarphlexin &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>What do you call a person who is outside a doo...</td>\n",
              "      <td>Matt</td>\n",
              "      <td>what do you call a person who is outside a doo...</td>\n",
              "      <td>&lt;start&gt; Matt &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; matt</td>\n",
              "      <td>matt &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Which Star Trek character is a member of the m...</td>\n",
              "      <td>Jean-Luc Pickacard</td>\n",
              "      <td>which star trek character is a member of the m...</td>\n",
              "      <td>&lt;start&gt; Jean-Luc Pickacard &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; jean luc pickacard</td>\n",
              "      <td>jean luc pickacard &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>What's the difference between a bullet and a h...</td>\n",
              "      <td>A bullet doesn't miss Harambe</td>\n",
              "      <td>what s the difference between a bullet and a h...</td>\n",
              "      <td>&lt;start&gt; A bullet doesn't miss Harambe &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; a bullet doesn t miss harambe</td>\n",
              "      <td>a bullet doesn t miss harambe &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  ...                       answer_preprocess_out\n",
              "0   1  ...  he nearly drown in his own tea pee . <end>\n",
              "1   2  ...                      mycheexarphlexin <end>\n",
              "2   3  ...                                  matt <end>\n",
              "3   4  ...                    jean luc pickacard <end>\n",
              "4   5  ...         a bullet doesn t miss harambe <end>\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXbLnhfB8Oaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ques_tokenizer=Tokenizer(filters='')\n",
        "ques_tokenizer.fit_on_texts(data['question_preprocess'])\n",
        "# inp_lan will have an integer represented for each word\n",
        "inp_lan=ques_tokenizer.texts_to_sequences(data['question_preprocess'])\n",
        "# Pad sequences (By default it will pad zeros till the max length of the ita_data)\n",
        "inp_lan=pad_sequences(inp_lan,padding='post')\n",
        "# Here inp_lan is the encoder input data."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e6rkuJ3-kYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declare the tokenizer for english words\n",
        "ans_tokenizer=Tokenizer(filters='')\n",
        "# we are fitting on eng_data not on eng_in or eng_out as it has both <start> and <end> tags\n",
        "ans_tokenizer.fit_on_texts(data['answer_preprocess'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-OYjlLD-kUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoder input data\n",
        "tar_lan_in=ans_tokenizer.texts_to_sequences(data['answer_preprocess_in'])\n",
        "tar_lan_in=pad_sequences(tar_lan_in,padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0RMaXhV-kSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoder output data\n",
        "tar_lan_out=ans_tokenizer.texts_to_sequences(data['answer_preprocess_out'])\n",
        "tar_lan_out=pad_sequences(tar_lan_out,padding='post')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx4XYR2r8N6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Max length of input_sequence, max_length of target sequence \n",
        "inp_seq_length=inp_lan[0].shape[0]\n",
        "tar_seq_length=tar_lan_in[0].shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBCCNw5X-wQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Total number of unique words in Italian sentence \n",
        "ques_vocab_size=len(ques_tokenizer.word_index)+1\n",
        "# Total number of unique words in english sentence\n",
        "ans_vocab_size=len(ans_tokenizer.word_index)+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA2gVlga-wNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Max length of input_sequence, max_length of target sequence \n",
        "inp_seq_length=inp_lan[0].shape[0]\n",
        "tar_seq_length=tar_lan_in[0].shape[0]\n",
        "\n",
        "ans_index_word={}\n",
        "ans_word_index={}\n",
        "for key,value in ans_tokenizer.word_index.items():\n",
        "  ans_index_word[value]=key \n",
        "  ans_word_index[key]=value\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zYMHCUJDMbW",
        "colab_type": "code",
        "outputId": "ada8c9ed-b65f-44a4-c911-b98a7f809fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ans_word_index"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<start>': 1,\n",
              " '<end>': 2,\n",
              " 'a': 3,\n",
              " 'the': 4,\n",
              " 'to': 5,\n",
              " 'because': 6,\n",
              " 'you': 7,\n",
              " 'they': 8,\n",
              " 'he': 9,\n",
              " 'it': 10,\n",
              " 'and': 11,\n",
              " 'of': 12,\n",
              " 'in': 13,\n",
              " 'i': 14,\n",
              " 'was': 15,\n",
              " 'is': 16,\n",
              " 'on': 17,\n",
              " 'his': 18,\n",
              " 'one': 19,\n",
              " \"it's\": 20,\n",
              " 'for': 21,\n",
              " 'have': 22,\n",
              " 'your': 23,\n",
              " 'with': 24,\n",
              " 'are': 25,\n",
              " 'get': 26,\n",
              " 'that': 27,\n",
              " 'my': 28,\n",
              " \"don't\": 29,\n",
              " 'just': 30,\n",
              " 'an': 31,\n",
              " 'can': 32,\n",
              " 'out': 33,\n",
              " 'be': 34,\n",
              " 'when': 35,\n",
              " 'all': 36,\n",
              " 'both': 37,\n",
              " 'but': 38,\n",
              " 'she': 39,\n",
              " 'at': 40,\n",
              " 'not': 41,\n",
              " 'if': 42,\n",
              " 'their': 43,\n",
              " \"they're\": 44,\n",
              " 'no': 45,\n",
              " 'up': 46,\n",
              " 'this': 47,\n",
              " 'her': 48,\n",
              " 'so': 49,\n",
              " 'has': 50,\n",
              " 'like': 51,\n",
              " 'them': 52,\n",
              " 'do': 53,\n",
              " \"can't\": 54,\n",
              " 'what': 55,\n",
              " 'other': 56,\n",
              " 'had': 57,\n",
              " 'by': 58,\n",
              " \"i'm\": 59,\n",
              " 'from': 60,\n",
              " 'it.': 61,\n",
              " 'me': 62,\n",
              " 'only': 63,\n",
              " 'always': 64,\n",
              " \"he's\": 65,\n",
              " 'got': 66,\n",
              " 'go': 67,\n",
              " 'will': 68,\n",
              " 'never': 69,\n",
              " 'how': 70,\n",
              " 'about': 71,\n",
              " 'there': 72,\n",
              " 'would': 73,\n",
              " 'him': 74,\n",
              " \"doesn't\": 75,\n",
              " 'know': 76,\n",
              " 'too': 77,\n",
              " 'little': 78,\n",
              " 'make': 79,\n",
              " 'were': 80,\n",
              " 'people': 81,\n",
              " 'two': 82,\n",
              " 'or': 83,\n",
              " 'who': 84,\n",
              " \"you're\": 85,\n",
              " 'take': 86,\n",
              " 'did': 87,\n",
              " 'want': 88,\n",
              " 'as': 89,\n",
              " 'see': 90,\n",
              " 'we': 91,\n",
              " 'joke': 92,\n",
              " 'really': 93,\n",
              " 'off': 94,\n",
              " 'good': 95,\n",
              " \"didn't\": 96,\n",
              " 'its': 97,\n",
              " 'into': 98,\n",
              " 'put': 99,\n",
              " 'down': 100,\n",
              " 'black': 101,\n",
              " 'some': 102,\n",
              " '.': 103,\n",
              " 'back': 104,\n",
              " 'time': 105,\n",
              " 'before': 106,\n",
              " 'tell': 107,\n",
              " 'say': 108,\n",
              " 'more': 109,\n",
              " 'man': 110,\n",
              " 'a:': 111,\n",
              " \"there's\": 112,\n",
              " 'call': 113,\n",
              " 'come': 114,\n",
              " 'could': 115,\n",
              " \"i'll\": 116,\n",
              " 'find': 117,\n",
              " 'why': 118,\n",
              " 'been': 119,\n",
              " 'over': 120,\n",
              " 'after': 121,\n",
              " 'then': 122,\n",
              " 'them.': 123,\n",
              " 'cause': 124,\n",
              " 'gets': 125,\n",
              " 'still': 126,\n",
              " 'made': 127,\n",
              " 'give': 128,\n",
              " 'first': 129,\n",
              " '-': 130,\n",
              " \"that's\": 131,\n",
              " 'going': 132,\n",
              " 'think': 133,\n",
              " 'white': 134,\n",
              " 'being': 135,\n",
              " 'dead': 136,\n",
              " 'already': 137,\n",
              " 'getting': 138,\n",
              " 'well': 139,\n",
              " 'right': 140,\n",
              " 'heard': 141,\n",
              " 'where': 142,\n",
              " 'edit:': 143,\n",
              " 'fucking': 144,\n",
              " 'than': 145,\n",
              " 'look': 146,\n",
              " \"couldn't\": 147,\n",
              " 'comes': 148,\n",
              " 'new': 149,\n",
              " 'shit': 150,\n",
              " 'now': 151,\n",
              " 'bad': 152,\n",
              " 'great': 153,\n",
              " 'said': 154,\n",
              " 'you.': 155,\n",
              " 'any': 156,\n",
              " 'around': 157,\n",
              " 'goes': 158,\n",
              " 'many': 159,\n",
              " 'every': 160,\n",
              " 'does': 161,\n",
              " 'out.': 162,\n",
              " 'need': 163,\n",
              " 'keep': 164,\n",
              " 'wanted': 165,\n",
              " 'fuck': 166,\n",
              " 'eat': 167,\n",
              " 'way': 168,\n",
              " 'came': 169,\n",
              " 'big': 170,\n",
              " 'am': 171,\n",
              " 'run': 172,\n",
              " 'told': 173,\n",
              " 'even': 174,\n",
              " 'use': 175,\n",
              " '\"i': 176,\n",
              " 'dick': 177,\n",
              " 'joke.': 178,\n",
              " 'trump': 179,\n",
              " 'hard': 180,\n",
              " \"i've\": 181,\n",
              " 'very': 182,\n",
              " 'work': 183,\n",
              " 'someone': 184,\n",
              " 'should': 185,\n",
              " 'old': 186,\n",
              " 'much': 187,\n",
              " \"won't\": 188,\n",
              " 'it,': 189,\n",
              " 'well,': 190,\n",
              " 'while': 191,\n",
              " ':': 192,\n",
              " 'thought': 193,\n",
              " 'year': 194,\n",
              " 'wife': 195,\n",
              " 'end': 196,\n",
              " 'called': 197,\n",
              " 'went': 198,\n",
              " 'same': 199,\n",
              " 'making': 200,\n",
              " 'through': 201,\n",
              " 'guy': 202,\n",
              " 'neither': 203,\n",
              " 'last': 204,\n",
              " 'between': 205,\n",
              " 'ask': 206,\n",
              " 'no,': 207,\n",
              " '...': 208,\n",
              " 'chicken': 209,\n",
              " 'small': 210,\n",
              " 'hear': 211,\n",
              " 'hit': 212,\n",
              " 'used': 213,\n",
              " 'having': 214,\n",
              " 'three': 215,\n",
              " 'another': 216,\n",
              " 'one.': 217,\n",
              " 'let': 218,\n",
              " 'pay': 219,\n",
              " 'none.': 220,\n",
              " 'left': 221,\n",
              " 'stop': 222,\n",
              " 'found': 223,\n",
              " 'change': 224,\n",
              " 'love': 225,\n",
              " 'until': 226,\n",
              " '2': 227,\n",
              " 'myself': 228,\n",
              " 'actually': 229,\n",
              " 'full': 230,\n",
              " 'up.': 231,\n",
              " 'dad': 232,\n",
              " 'none,': 233,\n",
              " 'long': 234,\n",
              " '1': 235,\n",
              " 'light': 236,\n",
              " 'everyone': 237,\n",
              " '3': 238,\n",
              " 'sure': 239,\n",
              " 'saw': 240,\n",
              " 'juan': 241,\n",
              " 'says': 242,\n",
              " 'nothing': 243,\n",
              " 'better': 244,\n",
              " 'turn': 245,\n",
              " 'name': 246,\n",
              " 'time.': 247,\n",
              " 'away': 248,\n",
              " 'day': 249,\n",
              " 'know,': 250,\n",
              " 'looking': 251,\n",
              " 'nobody': 252,\n",
              " \"they'll\": 253,\n",
              " 'back.': 254,\n",
              " 'jokes': 255,\n",
              " 'head': 256,\n",
              " 'something': 257,\n",
              " 'donald': 258,\n",
              " 'leave': 259,\n",
              " 'under': 260,\n",
              " 'high': 261,\n",
              " 'ever': 262,\n",
              " 'lot': 263,\n",
              " 'sex': 264,\n",
              " 'enough': 265,\n",
              " 'play': 266,\n",
              " 'those': 267,\n",
              " 'buy': 268,\n",
              " 'kill': 269,\n",
              " 'wants': 270,\n",
              " 'sorry': 271,\n",
              " 'here': 272,\n",
              " \"she's\": 273,\n",
              " 'nuts': 274,\n",
              " 'throw': 275,\n",
              " 'trying': 276,\n",
              " 'once': 277,\n",
              " 'woman': 278,\n",
              " 'makes': 279,\n",
              " 'women': 280,\n",
              " 'dog': 281,\n",
              " \"wouldn't\": 282,\n",
              " 'try': 283,\n",
              " \"wasn't\": 284,\n",
              " 'gonna': 285,\n",
              " 'mom': 286,\n",
              " 'him.': 287,\n",
              " 'half': 288,\n",
              " 'now.': 289,\n",
              " 'taste': 290,\n",
              " 'nothing.': 291,\n",
              " 'god': 292,\n",
              " 'post': 293,\n",
              " 'america': 294,\n",
              " 'hold': 295,\n",
              " 'pretty': 296,\n",
              " 'nothing,': 297,\n",
              " 'next': 298,\n",
              " 'men': 299,\n",
              " 'family': 300,\n",
              " 'me.': 301,\n",
              " \"what's\": 302,\n",
              " 'pull': 303,\n",
              " 'own': 304,\n",
              " 'off.': 305,\n",
              " 'show': 306,\n",
              " 'asked': 307,\n",
              " 'ones': 308,\n",
              " 'start': 309,\n",
              " 'watch': 310,\n",
              " 'ass': 311,\n",
              " 'house': 312,\n",
              " 'takes': 313,\n",
              " 'car': 314,\n",
              " 'screw': 315,\n",
              " 'real': 316,\n",
              " 'inside': 317,\n",
              " 'probably': 318,\n",
              " 'shit.': 319,\n",
              " 'lost': 320,\n",
              " \"i'd\": 321,\n",
              " 'turns': 322,\n",
              " 'anything': 323,\n",
              " 'side': 324,\n",
              " 'whole': 325,\n",
              " 'credit': 326,\n",
              " 'please': 327,\n",
              " '6': 328,\n",
              " 'four': 329,\n",
              " 'money': 330,\n",
              " \"one's\": 331,\n",
              " 'blow': 332,\n",
              " 'running': 333,\n",
              " 'fall': 334,\n",
              " 'red': 335,\n",
              " \"isn't\": 336,\n",
              " 'world': 337,\n",
              " 'took': 338,\n",
              " 'side.': 339,\n",
              " 'one,': 340,\n",
              " 'gas': 341,\n",
              " 'second': 342,\n",
              " 'seen': 343,\n",
              " 'free': 344,\n",
              " 'wrong': 345,\n",
              " 'face': 346,\n",
              " 'open': 347,\n",
              " 'nice': 348,\n",
              " 'wanna': 349,\n",
              " 'hot': 350,\n",
              " 'crack': 351,\n",
              " 'police': 352,\n",
              " 'home': 353,\n",
              " 'front': 354,\n",
              " 'cuz': 355,\n",
              " '\"': 356,\n",
              " 'feel': 357,\n",
              " 'might': 358,\n",
              " 'which': 359,\n",
              " 'million': 360,\n",
              " 'face.': 361,\n",
              " 'oh': 362,\n",
              " 'six': 363,\n",
              " 'life': 364,\n",
              " 'years': 365,\n",
              " 'day.': 366,\n",
              " 'baby': 367,\n",
              " 'hell': 368,\n",
              " 'person': 369,\n",
              " 'beat': 370,\n",
              " 'behind': 371,\n",
              " 'things': 372,\n",
              " 'best': 373,\n",
              " 'cut': 374,\n",
              " 'our': 375,\n",
              " 'without': 376,\n",
              " 'these': 377,\n",
              " '\"you': 378,\n",
              " '[removed]': 379,\n",
              " 'least': 380,\n",
              " 'kept': 381,\n",
              " \"hasn't\": 382,\n",
              " 'pizza': 383,\n",
              " 'q:': 384,\n",
              " 'me:': 385,\n",
              " 'talk': 386,\n",
              " 'man.': 387,\n",
              " 'none': 388,\n",
              " 'said,': 389,\n",
              " 'night': 390,\n",
              " 'each': 391,\n",
              " 'most': 392,\n",
              " \"they'd\": 393,\n",
              " 'ate': 394,\n",
              " \"haven't\": 395,\n",
              " '10': 396,\n",
              " 'coming': 397,\n",
              " 'mean': 398,\n",
              " 'hole': 399,\n",
              " 'kick': 400,\n",
              " 'reddit': 401,\n",
              " 'santa': 402,\n",
              " 'line': 403,\n",
              " 'room': 404,\n",
              " 'black.': 405,\n",
              " 'shoot': 406,\n",
              " 'on.': 407,\n",
              " 'finish': 408,\n",
              " \"other's\": 409,\n",
              " 'blue': 410,\n",
              " 'stand': 411,\n",
              " 'everything': 412,\n",
              " 'game': 413,\n",
              " 'fit': 414,\n",
              " 'piece': 415,\n",
              " '(': 416,\n",
              " 'says,': 417,\n",
              " 'depends': 418,\n",
              " 'bill': 419,\n",
              " 'racist': 420,\n",
              " 'cow': 421,\n",
              " 'kids': 422,\n",
              " '!': 423,\n",
              " 'america.': 424,\n",
              " 'medium': 425,\n",
              " 'us': 426,\n",
              " 'na': 427,\n",
              " '8': 428,\n",
              " 'must': 429,\n",
              " 'thanks': 430,\n",
              " 'bus': 431,\n",
              " 'air': 432,\n",
              " \"they've\": 433,\n",
              " 'jesus': 434,\n",
              " 'guys': 435,\n",
              " 'ass.': 436,\n",
              " 'way.': 437,\n",
              " '7': 438,\n",
              " 'yet.': 439,\n",
              " 'happy': 440,\n",
              " 'son': 441,\n",
              " 'in.': 442,\n",
              " '\"i\\'m': 443,\n",
              " 'apparently': 444,\n",
              " 'cover': 445,\n",
              " 'top': 446,\n",
              " 'sorry.': 447,\n",
              " 'stuck': 448,\n",
              " 'middle': 449,\n",
              " 'walk': 450,\n",
              " 'ice': 451,\n",
              " 'drink': 452,\n",
              " 'catch': 453,\n",
              " 'it!': 454,\n",
              " 'dead.': 455,\n",
              " \"you've\": 456,\n",
              " 'again.': 457,\n",
              " 'jews': 458,\n",
              " 'bit': 459,\n",
              " 'girl': 460,\n",
              " 'funny': 461,\n",
              " 'bean': 462,\n",
              " 'guess': 463,\n",
              " 'balls': 464,\n",
              " 'believe': 465,\n",
              " 'hillary': 466,\n",
              " 'thing': 467,\n",
              " \"ain't\": 468,\n",
              " 'there.': 469,\n",
              " 'teacher:': 470,\n",
              " 'student:': 471,\n",
              " \"let's\": 472,\n",
              " 'know.': 473,\n",
              " 'bulb': 474,\n",
              " 'orange': 475,\n",
              " 'may': 476,\n",
              " 'school': 477,\n",
              " 'anyone': 478,\n",
              " 'sit': 479,\n",
              " 'few': 480,\n",
              " '&': 481,\n",
              " 'times': 482,\n",
              " 'live': 483,\n",
              " 'afraid': 484,\n",
              " 'green': 485,\n",
              " 'wall': 486,\n",
              " ':)': 487,\n",
              " 'american': 488,\n",
              " 'child': 489,\n",
              " 'no?': 490,\n",
              " 'deep': 491,\n",
              " 'suck': 492,\n",
              " 'knock': 493,\n",
              " 'tea': 494,\n",
              " 'oven.': 495,\n",
              " '4': 496,\n",
              " 'cancer': 497,\n",
              " 'gotta': 498,\n",
              " 'dark': 499,\n",
              " 'ten': 500,\n",
              " 'thank': 501,\n",
              " 'race.': 502,\n",
              " 'job': 503,\n",
              " 'hand': 504,\n",
              " 'doing': 505,\n",
              " 'cold': 506,\n",
              " 'jack': 507,\n",
              " 'five': 508,\n",
              " 'died': 509,\n",
              " 'cross': 510,\n",
              " 'virgin': 511,\n",
              " 'meat': 512,\n",
              " 'fly': 513,\n",
              " ')': 514,\n",
              " 'salad': 515,\n",
              " 'bear': 516,\n",
              " 'needed': 517,\n",
              " 'word': 518,\n",
              " 'rest': 519,\n",
              " 'parents': 520,\n",
              " 'im': 521,\n",
              " 'group': 522,\n",
              " '5': 523,\n",
              " 'husband': 524,\n",
              " 'fire': 525,\n",
              " 'felt': 526,\n",
              " 'nail': 527,\n",
              " 'hang': 528,\n",
              " 'yes': 529,\n",
              " 'far': 530,\n",
              " 'playing': 531,\n",
              " 'either': 532,\n",
              " 'jokes.': 533,\n",
              " 'read': 534,\n",
              " 'dog.': 535,\n",
              " 'food': 536,\n",
              " 'mouth': 537,\n",
              " '9': 538,\n",
              " 'keeps': 539,\n",
              " 'dirty': 540,\n",
              " ',': 541,\n",
              " 'smell': 542,\n",
              " 'up!': 543,\n",
              " 'difference': 544,\n",
              " 'sorry,': 545,\n",
              " 'truck': 546,\n",
              " 'also': 547,\n",
              " 'shot': 548,\n",
              " 'bunch': 549,\n",
              " 'jew': 550,\n",
              " 'again': 551,\n",
              " '20': 552,\n",
              " 'cancer.': 553,\n",
              " ':d': 554,\n",
              " 'people.': 555,\n",
              " 'lol': 556,\n",
              " 'matter': 557,\n",
              " 'nun': 558,\n",
              " 'fat': 559,\n",
              " 'damn': 560,\n",
              " 'drop': 561,\n",
              " 'tickles': 562,\n",
              " 'land': 563,\n",
              " 'too.': 564,\n",
              " \"we're\": 565,\n",
              " 'hands': 566,\n",
              " 'garbanzo': 567,\n",
              " 'kid': 568,\n",
              " 'close': 569,\n",
              " 'likes': 570,\n",
              " 'gave': 571,\n",
              " 'hide': 572,\n",
              " 'head.': 573,\n",
              " 'looks': 574,\n",
              " 'case': 575,\n",
              " 'means': 576,\n",
              " 'against': 577,\n",
              " 'hair': 578,\n",
              " 'punch': 579,\n",
              " 'busty': 580,\n",
              " 'eating': 581,\n",
              " 'father': 582,\n",
              " 'her.': 583,\n",
              " 'different': 584,\n",
              " 'bring': 585,\n",
              " 'lit': 586,\n",
              " \"aren't\": 587,\n",
              " 'doctor': 588,\n",
              " 'killed': 589,\n",
              " 'able': 590,\n",
              " 'cunning': 591,\n",
              " 'boy': 592,\n",
              " 'dick.': 593,\n",
              " 'dont': 594,\n",
              " 'fell': 595,\n",
              " 'question': 596,\n",
              " 'michael': 597,\n",
              " 'become': 598,\n",
              " 'dad:': 599,\n",
              " 'large': 600,\n",
              " 'usually': 601,\n",
              " 'line.': 602,\n",
              " 'beef': 603,\n",
              " 'period': 604,\n",
              " 'waits': 605,\n",
              " 'caught': 606,\n",
              " 'jewish': 607,\n",
              " 'answer': 608,\n",
              " 'knows': 609,\n",
              " 'there?': 610,\n",
              " 'chicken.': 611,\n",
              " 'third': 612,\n",
              " 'drive': 613,\n",
              " 'part': 614,\n",
              " 'friend': 615,\n",
              " 'bill.': 616,\n",
              " 'taking': 617,\n",
              " 'elephant': 618,\n",
              " 'girls': 619,\n",
              " 'straight': 620,\n",
              " 'understand': 621,\n",
              " 'me,': 622,\n",
              " 'roll': 623,\n",
              " 'course.': 624,\n",
              " 'dark.': 625,\n",
              " 'life.': 626,\n",
              " 'legs': 627,\n",
              " 'job.': 628,\n",
              " 'morning': 629,\n",
              " \"you'll\": 630,\n",
              " 'hate': 631,\n",
              " 'minutes': 632,\n",
              " 'yes,': 633,\n",
              " 'fish': 634,\n",
              " 'blood': 635,\n",
              " 'crusty': 636,\n",
              " 'nailed': 637,\n",
              " 'laid': 638,\n",
              " \"you'd\": 639,\n",
              " 'else': 640,\n",
              " 'lose': 641,\n",
              " '?': 642,\n",
              " 'help': 643,\n",
              " 'son:': 644,\n",
              " 'french': 645,\n",
              " 'hope': 646,\n",
              " 'spend': 647,\n",
              " 'ran': 648,\n",
              " 'unless': 649,\n",
              " 'saying': 650,\n",
              " 'sound': 651,\n",
              " 'bernie': 652,\n",
              " 'away.': 653,\n",
              " 'pool': 654,\n",
              " 'hard.': 655,\n",
              " 'store': 656,\n",
              " 'everywhere.': 657,\n",
              " 'drunk': 658,\n",
              " 'sell': 659,\n",
              " 'ugly': 660,\n",
              " '=': 661,\n",
              " 'break': 662,\n",
              " 'wall.': 663,\n",
              " 'sea': 664,\n",
              " 'yeah,': 665,\n",
              " 'number': 666,\n",
              " 'turned': 667,\n",
              " 'eye': 668,\n",
              " 'girlfriend': 669,\n",
              " 'done': 670,\n",
              " '9/11': 671,\n",
              " 'broke': 672,\n",
              " 'load': 673,\n",
              " 'stops': 674,\n",
              " 'working': 675,\n",
              " 'this.': 676,\n",
              " 'grass': 677,\n",
              " 'gives': 678,\n",
              " 'themselves': 679,\n",
              " 'swim': 680,\n",
              " 'push': 681,\n",
              " 'asshole': 682,\n",
              " 'mother': 683,\n",
              " 'vote': 684,\n",
              " 'yet': 685,\n",
              " 'well.': 686,\n",
              " 'point': 687,\n",
              " 'mad': 688,\n",
              " 'bomb': 689,\n",
              " 'jump': 690,\n",
              " 'short': 691,\n",
              " 'steal': 692,\n",
              " 'pick': 693,\n",
              " 'cool.': 694,\n",
              " 'finding': 695,\n",
              " 'tampon': 696,\n",
              " 'once.': 697,\n",
              " \"who's\": 698,\n",
              " 'year.': 699,\n",
              " 'deal': 700,\n",
              " 'wife:': 701,\n",
              " 'two.': 702,\n",
              " 'right.': 703,\n",
              " 'becomes': 704,\n",
              " 'christian': 705,\n",
              " 'talking': 706,\n",
              " 'forgot': 707,\n",
              " \"'cause\": 708,\n",
              " 'hose': 709,\n",
              " 'ash': 710,\n",
              " 'down.': 711,\n",
              " 'die': 712,\n",
              " 'frog': 713,\n",
              " 'throat.': 714,\n",
              " 'seven': 715,\n",
              " 'is.': 716,\n",
              " '\"no,': 717,\n",
              " 'shot.': 718,\n",
              " 'water': 719,\n",
              " 'ant': 720,\n",
              " 'large.': 721,\n",
              " 'stay': 722,\n",
              " 'country': 723,\n",
              " 'friends': 724,\n",
              " 'stopped': 725,\n",
              " 'forget': 726,\n",
              " 'steak': 727,\n",
              " 'change.': 728,\n",
              " 'watching': 729,\n",
              " 'box': 730,\n",
              " 'mexicans': 731,\n",
              " 'history': 732,\n",
              " \"weren't\": 733,\n",
              " 'twice': 734,\n",
              " 'sick': 735,\n",
              " 'twenty': 736,\n",
              " 'horse': 737,\n",
              " 'apple': 738,\n",
              " 'alone': 739,\n",
              " 'polar': 740,\n",
              " 'runs': 741,\n",
              " 'standing': 742,\n",
              " 'enjoy': 743,\n",
              " 'ride': 744,\n",
              " 'you,': 745,\n",
              " 'shucks': 746,\n",
              " 'pulled': 747,\n",
              " 'outstanding': 748,\n",
              " 'b:': 749,\n",
              " 'work.': 750,\n",
              " 'wet': 751,\n",
              " 'place': 752,\n",
              " 'fart': 753,\n",
              " 'add': 754,\n",
              " 'crisis.': 755,\n",
              " 'since': 756,\n",
              " 'sitting': 757,\n",
              " 'lightbulb.': 758,\n",
              " 'literally': 759,\n",
              " 'bird': 760,\n",
              " 'lay': 761,\n",
              " 'paint': 762,\n",
              " 'sister': 763,\n",
              " 'chimney.': 764,\n",
              " 'prefer': 765,\n",
              " 'sleeping': 766,\n",
              " 'check': 767,\n",
              " '\"it\\'s': 768,\n",
              " 'front,': 769,\n",
              " 'field.': 770,\n",
              " 'them,': 771,\n",
              " 'nazi': 772,\n",
              " 'box.': 773,\n",
              " 'potato': 774,\n",
              " 'it?': 775,\n",
              " 'nine': 776,\n",
              " 'dogs': 777,\n",
              " 'hey': 778,\n",
              " 'couple': 779,\n",
              " 'house.': 780,\n",
              " 'condescending': 781,\n",
              " 'white.': 782,\n",
              " 'blew': 783,\n",
              " 'sir': 784,\n",
              " 'terrible': 785,\n",
              " 'cum': 786,\n",
              " 'double': 787,\n",
              " 'fast': 788,\n",
              " 'trump.': 789,\n",
              " 'support': 790,\n",
              " 'low': 791,\n",
              " 'years.': 792,\n",
              " 'rather': 793,\n",
              " 'cantaloupe.': 794,\n",
              " 'balls.': 795,\n",
              " 'hey,': 796,\n",
              " 'first.': 797,\n",
              " 'calling': 798,\n",
              " 'suicide': 799,\n",
              " 'b': 800,\n",
              " 'age': 801,\n",
              " 'sign': 802,\n",
              " 'repost': 803,\n",
              " 'tuna': 804,\n",
              " 'ha': 805,\n",
              " 'flag': 806,\n",
              " 'such': 807,\n",
              " 'cat': 808,\n",
              " 'mind': 809,\n",
              " 'together': 810,\n",
              " 'husband:': 811,\n",
              " 'fingers': 812,\n",
              " 'denim': 813,\n",
              " 'fine': 814,\n",
              " 'bar': 815,\n",
              " 'miss': 816,\n",
              " 'bed': 817,\n",
              " 'gay': 818,\n",
              " 'slow': 819,\n",
              " 'fire.': 820,\n",
              " 'uses': 821,\n",
              " 'huge': 822,\n",
              " 'eggs': 823,\n",
              " 'deer': 824,\n",
              " 'beer': 825,\n",
              " 'driver': 826,\n",
              " 'tried': 827,\n",
              " 'christmas': 828,\n",
              " 'outside.': 829,\n",
              " 'children': 830,\n",
              " 'knee': 831,\n",
              " 'door.': 832,\n",
              " '100': 833,\n",
              " 'lightbulb': 834,\n",
              " 'asking': 835,\n",
              " 'course': 836,\n",
              " '\"the': 837,\n",
              " 'easy': 838,\n",
              " 'it’s': 839,\n",
              " 'brown': 840,\n",
              " 'two,': 841,\n",
              " 'hispanic': 842,\n",
              " 'wash': 843,\n",
              " 'telling': 844,\n",
              " 'gay.': 845,\n",
              " 'carry': 846,\n",
              " 'ring': 847,\n",
              " 'r': 848,\n",
              " 'flip': 849,\n",
              " '\"what': 850,\n",
              " 'pain': 851,\n",
              " 'joke,': 852,\n",
              " 'out,': 853,\n",
              " 'wondering': 854,\n",
              " 'plane': 855,\n",
              " 'said:': 856,\n",
              " 'to.': 857,\n",
              " 'shooting': 858,\n",
              " 'meet': 859,\n",
              " 'baked': 860,\n",
              " 'kids.': 861,\n",
              " 'snatches': 862,\n",
              " 'stick': 863,\n",
              " 'paid': 864,\n",
              " 'ok': 865,\n",
              " 'starts': 866,\n",
              " 'legs.': 867,\n",
              " 'snow': 868,\n",
              " 'male': 869,\n",
              " 'computer': 870,\n",
              " 'holy': 871,\n",
              " 'delivery.': 872,\n",
              " 'swallow': 873,\n",
              " 'wearing': 874,\n",
              " 'law': 875,\n",
              " 'born': 876,\n",
              " 'giving': 877,\n",
              " 'poor': 878,\n",
              " 'all.': 879,\n",
              " 'remove': 880,\n",
              " 'racist.': 881,\n",
              " 'pasta': 882,\n",
              " 'feed': 883,\n",
              " 'clinton': 884,\n",
              " 'tired.': 885,\n",
              " 'during': 886,\n",
              " 'salad.': 887,\n",
              " 'eyes': 888,\n",
              " 'build': 889,\n",
              " 'name.': 890,\n",
              " 'broken': 891,\n",
              " 'across': 892,\n",
              " 'young': 893,\n",
              " 'arms.': 894,\n",
              " 'mary': 895,\n",
              " 'tree': 896,\n",
              " 'car,': 897,\n",
              " 'foot': 898,\n",
              " 'olive': 899,\n",
              " 'step': 900,\n",
              " 'back,': 901,\n",
              " 'cant': 902,\n",
              " 'pricks': 903,\n",
              " 'bear.': 904,\n",
              " 'dr.': 905,\n",
              " 'aids': 906,\n",
              " 'acne': 907,\n",
              " 'follow': 908,\n",
              " 'chance': 909,\n",
              " 'tastes': 910,\n",
              " 'hearing': 911,\n",
              " 'inch': 912,\n",
              " 'camp.': 913,\n",
              " 'harambe': 914,\n",
              " 'sees': 915,\n",
              " 'scream': 916,\n",
              " 'bach': 917,\n",
              " 'field': 918,\n",
              " 'thinks': 919,\n",
              " 'nose': 920,\n",
              " 'nuts.': 921,\n",
              " 'touch': 922,\n",
              " '10%': 923,\n",
              " 'woke': 924,\n",
              " 'asks': 925,\n",
              " 'sandy': 926,\n",
              " 'ground': 927,\n",
              " 'hardware': 928,\n",
              " 'problem.': 929,\n",
              " 'care.': 930,\n",
              " 'super': 931,\n",
              " '\"you\\'re': 932,\n",
              " 'instead': 933,\n",
              " 'bottom': 934,\n",
              " 'walked': 935,\n",
              " 'reach': 936,\n",
              " 'feet': 937,\n",
              " 'done.': 938,\n",
              " 'hebrews': 939,\n",
              " 'da': 940,\n",
              " 'putin': 941,\n",
              " 'drinks': 942,\n",
              " 'car.': 943,\n",
              " 'watches': 944,\n",
              " 'liquor': 945,\n",
              " 'body': 946,\n",
              " 'shoes': 947,\n",
              " 'can.': 948,\n",
              " 'stays': 949,\n",
              " 'worry,': 950,\n",
              " 'others': 951,\n",
              " 'today': 952,\n",
              " 'chinese': 953,\n",
              " 'named': 954,\n",
              " 'mouth.': 955,\n",
              " 'harambe.': 956,\n",
              " 'save': 957,\n",
              " 'school.': 958,\n",
              " 'out...': 959,\n",
              " 'before.': 960,\n",
              " '12': 961,\n",
              " 'longer': 962,\n",
              " 'shoulders': 963,\n",
              " '͡°': 964,\n",
              " '͜ʖ': 965,\n",
              " '͡°)': 966,\n",
              " 'con': 967,\n",
              " 'ball.': 968,\n",
              " 'rock': 969,\n",
              " 'missing': 970,\n",
              " 'you!': 971,\n",
              " 'smells': 972,\n",
              " 'no.': 973,\n",
              " 'wheelchair': 974,\n",
              " 'hits': 975,\n",
              " 'living': 976,\n",
              " 'using': 977,\n",
              " 'past': 978,\n",
              " 'feeling': 979,\n",
              " 'reading': 980,\n",
              " 'pair': 981,\n",
              " 'sleep': 982,\n",
              " '50': 983,\n",
              " 'move': 984,\n",
              " 'president': 985,\n",
              " 'mexico': 986,\n",
              " 'human': 987,\n",
              " 'john': 988,\n",
              " 'sucks': 989,\n",
              " 'udder': 990,\n",
              " 'wait': 991,\n",
              " 'test': 992,\n",
              " 'bike': 993,\n",
              " 'cheap': 994,\n",
              " 'lead': 995,\n",
              " 'home.': 996,\n",
              " 'pissed': 997,\n",
              " \"he'd\": 998,\n",
              " 'son,': 999,\n",
              " 'posted': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqE8aZBq-wLL",
        "colab_type": "code",
        "outputId": "871da22c-9d84-4e35-9a8a-c8518ee67339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "print('input lang in',inp_lan.shape)\n",
        "print('target lang in',tar_lan_in.shape)\n",
        "print('target lang out',tar_lan_out.shape)\n",
        "\n",
        "# Total number of unique words in Italian sentence \n",
        "ques_vocab_size=len(ques_tokenizer.word_index)+1\n",
        "# Total number of unique words in english sentence\n",
        "ans_vocab_size=len(ans_tokenizer.word_index)+1\n",
        "\n",
        "print('question vocab size',ques_vocab_size)\n",
        "print('answer vocab size',ans_vocab_size)\n",
        "#print(ita_tokenizer_in.word_index)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input lang in (15000, 26)\n",
            "target lang in (15000, 112)\n",
            "target lang out (15000, 112)\n",
            "question vocab size 10672\n",
            "answer vocab size 18979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWVkXuG6-wIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsaoA29X-wGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Encoder input shape will be (Batch_size,inp_seq_length) --\n",
        "input_encoder=Input(shape=(inp_seq_length,))\n",
        "\n",
        "# Encoder LSTM needs the input to be 3 dim. One easy way to add a dimension to this integer sequences is to add a Embedding layer.\n",
        "\n",
        "# input_encoder is passed to the embedding layer.  \n",
        "encoder_embedding=Embedding(input_dim=ques_vocab_size,output_dim=128)(input_encoder)\n",
        "# encoder_embedding shape is (Batch_size,inp_seq_length,output_dim) --\n",
        "# Trainable paramters is ita_vocab_size * 128. \n",
        "# Each integer in the input_encoder is converted to 128 dimensional vector\n",
        "\n",
        "\n",
        "encoder_lstm=LSTM(256,return_state=True)(encoder_embedding)\n",
        "# Encoder_LSTM will return three arrays \n",
        "# 1. hidden_state/output of the last time step (None,256), \n",
        "# 2. hidden_state/ output of the last time step(None,256), \n",
        "# 2. cell_state of the last time step (None,256)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOFs86ee-wDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_states=encoder_lstm[1:]\n",
        "# Select the last two hidden states."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdIB9bZq-wCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoder input, we will keep the shape (None). Note: If none it can accept any size of data in dimension\n",
        "# The reason we are not fixing the input_decoder to max_length is during \n",
        "# inference we will be sending only one input at a time step and predict. \n",
        "\n",
        "input_decoder = Input(shape=(None,))\n",
        "# input_decoder shape is (None,None)\n",
        "\n",
        "decoder_embedding_layer=Embedding(input_dim=ans_vocab_size,output_dim=128)\n",
        "decoder_embedding_output=decoder_embedding_layer(input_decoder)\n",
        "# decoder_embedding_layer - (None,None,128) \n",
        "\n",
        "decoder_lstm=LSTM(256,return_sequences=True,return_state=True)\n",
        "decoder_output,_,_=decoder_lstm(decoder_embedding_output,initial_state=encoder_states)\n",
        "# decoder_lstm will return three outputs\n",
        "# 1. All the hiddenstate/output at each time step (None,None,256)\n",
        "# 2. hiddenstate/output at last time step (None,256)\n",
        "# 3. Cell state at last time step (None,256)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux9gv0tn-wA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply dense layer(units=eng_vocab_size) at every time step of the decoder_output\n",
        "decoder_dense=TimeDistributed( Dense(ans_vocab_size,activation='softmax'))\n",
        "dense_output= decoder_dense(decoder_output)\n",
        "# dense_output shape (None,none, eng_vocab_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIIjENSB-v-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our model has two inputs, one to the encoder and other to the decoder. \n",
        "# outputs - Dense layer output\n",
        "model=Model([input_encoder,input_decoder],dense_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09a8GwI4-v4h",
        "colab_type": "code",
        "outputId": "f7e85e83-7015-414e-d4da-5f1796e2e81f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 26)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 26, 128)      1366016     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 128)    2429312     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 256), (None, 394240      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 18979)  4877603     lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 9,461,411\n",
            "Trainable params: 9,461,411\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm4SMFUd-v2W",
        "colab_type": "code",
        "outputId": "42d145f3-aa71-4454-8b88-2b6264ea7edf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# if input_decoder shape is (None,max_length)\n",
        "# Dense output shape is (None,max_length,eng_vocab_size), decoder_output (None,max_length)\n",
        "# Target data is just integers, while predicted data is a softmax output\n",
        "# we apply sparse_categorical_crossentropy instead of categorical_crossentropy\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "# Train the model\n",
        "model.fit([inp_lan, tar_lan_in], tar_lan_out,\n",
        "          batch_size=64,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)  "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "188/188 [==============================] - 58s 310ms/step - loss: 0.7472 - val_loss: 0.4054\n",
            "Epoch 2/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.3916 - val_loss: 0.3855\n",
            "Epoch 3/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.3734 - val_loss: 0.3732\n",
            "Epoch 4/50\n",
            "188/188 [==============================] - 58s 307ms/step - loss: 0.3595 - val_loss: 0.3657\n",
            "Epoch 5/50\n",
            "188/188 [==============================] - 58s 307ms/step - loss: 0.3492 - val_loss: 0.3616\n",
            "Epoch 6/50\n",
            "188/188 [==============================] - 58s 307ms/step - loss: 0.3414 - val_loss: 0.3579\n",
            "Epoch 7/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.3335 - val_loss: 0.3550\n",
            "Epoch 8/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.3266 - val_loss: 0.3534\n",
            "Epoch 9/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.3203 - val_loss: 0.3529\n",
            "Epoch 10/50\n",
            "188/188 [==============================] - 58s 307ms/step - loss: 0.3145 - val_loss: 0.3517\n",
            "Epoch 11/50\n",
            "188/188 [==============================] - 58s 307ms/step - loss: 0.3084 - val_loss: 0.3502\n",
            "Epoch 12/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.3029 - val_loss: 0.3497\n",
            "Epoch 13/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2978 - val_loss: 0.3510\n",
            "Epoch 14/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2919 - val_loss: 0.3497\n",
            "Epoch 15/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2864 - val_loss: 0.3510\n",
            "Epoch 16/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2812 - val_loss: 0.3508\n",
            "Epoch 17/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2770 - val_loss: 0.3516\n",
            "Epoch 18/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2714 - val_loss: 0.3522\n",
            "Epoch 19/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2663 - val_loss: 0.3532\n",
            "Epoch 20/50\n",
            "188/188 [==============================] - 58s 307ms/step - loss: 0.2614 - val_loss: 0.3548\n",
            "Epoch 21/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2562 - val_loss: 0.3559\n",
            "Epoch 22/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2513 - val_loss: 0.3565\n",
            "Epoch 23/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2467 - val_loss: 0.3586\n",
            "Epoch 24/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2420 - val_loss: 0.3606\n",
            "Epoch 25/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2371 - val_loss: 0.3628\n",
            "Epoch 26/50\n",
            "188/188 [==============================] - 58s 307ms/step - loss: 0.2323 - val_loss: 0.3642\n",
            "Epoch 27/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2279 - val_loss: 0.3649\n",
            "Epoch 28/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2236 - val_loss: 0.3680\n",
            "Epoch 29/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2191 - val_loss: 0.3700\n",
            "Epoch 30/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2154 - val_loss: 0.3721\n",
            "Epoch 31/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2108 - val_loss: 0.3739\n",
            "Epoch 32/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2071 - val_loss: 0.3756\n",
            "Epoch 33/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.2030 - val_loss: 0.3789\n",
            "Epoch 34/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.1992 - val_loss: 0.3808\n",
            "Epoch 35/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.1951 - val_loss: 0.3828\n",
            "Epoch 36/50\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.1912 - val_loss: 0.3853\n",
            "Epoch 37/50\n",
            "188/188 [==============================] - 57s 306ms/step - loss: 0.1874 - val_loss: 0.3866\n",
            "Epoch 38/50\n",
            "188/188 [==============================] - 58s 308ms/step - loss: 0.1837 - val_loss: 0.3893\n",
            "Epoch 39/50\n",
            "188/188 [==============================] - 58s 308ms/step - loss: 0.1801 - val_loss: 0.3907\n",
            "Epoch 40/50\n",
            "188/188 [==============================] - 58s 307ms/step - loss: 0.1761 - val_loss: 0.3943\n",
            "Epoch 41/50\n",
            "188/188 [==============================] - 58s 308ms/step - loss: 0.1730 - val_loss: 0.3971\n",
            "Epoch 42/50\n",
            "188/188 [==============================] - 58s 308ms/step - loss: 0.1695 - val_loss: 0.3984\n",
            "Epoch 43/50\n",
            "188/188 [==============================] - 58s 308ms/step - loss: 0.1662 - val_loss: 0.4018\n",
            "Epoch 44/50\n",
            "188/188 [==============================] - 58s 307ms/step - loss: 0.1628 - val_loss: 0.4038\n",
            "Epoch 45/50\n",
            "188/188 [==============================] - 58s 307ms/step - loss: 0.1599 - val_loss: 0.4052\n",
            "Epoch 46/50\n",
            "188/188 [==============================] - 58s 307ms/step - loss: 0.1568 - val_loss: 0.4076\n",
            "Epoch 47/50\n",
            "188/188 [==============================] - 58s 308ms/step - loss: 0.1536 - val_loss: 0.4097\n",
            "Epoch 48/50\n",
            "188/188 [==============================] - 58s 308ms/step - loss: 0.1504 - val_loss: 0.4127\n",
            "Epoch 49/50\n",
            "188/188 [==============================] - 58s 308ms/step - loss: 0.1476 - val_loss: 0.4148\n",
            "Epoch 50/50\n",
            "188/188 [==============================] - 58s 308ms/step - loss: 0.1449 - val_loss: 0.4178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa0505354a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMFCs7vpfjOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "outputId": "d2a4098f-0e0f-4853-a0dd-6b168916ec09"
      },
      "source": [
        "model.fit([inp_lan, tar_lan_in], tar_lan_out,\n",
        "          batch_size=64,\n",
        "          epochs=20,\n",
        "          validation_split=0.2)  "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.1172 - val_loss: 0.4398\n",
            "Epoch 2/20\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.1152 - val_loss: 0.4421\n",
            "Epoch 3/20\n",
            "188/188 [==============================] - 58s 307ms/step - loss: 0.1128 - val_loss: 0.4428\n",
            "Epoch 4/20\n",
            "188/188 [==============================] - 57s 306ms/step - loss: 0.1111 - val_loss: 0.4444\n",
            "Epoch 5/20\n",
            "188/188 [==============================] - 57s 305ms/step - loss: 0.1089 - val_loss: 0.4462\n",
            "Epoch 6/20\n",
            "188/188 [==============================] - 57s 305ms/step - loss: 0.1067 - val_loss: 0.4487\n",
            "Epoch 7/20\n",
            "188/188 [==============================] - 57s 306ms/step - loss: 0.1049 - val_loss: 0.4494\n",
            "Epoch 8/20\n",
            "188/188 [==============================] - 57s 305ms/step - loss: 0.1029 - val_loss: 0.4524\n",
            "Epoch 9/20\n",
            "188/188 [==============================] - 57s 305ms/step - loss: 0.1012 - val_loss: 0.4539\n",
            "Epoch 10/20\n",
            "188/188 [==============================] - 57s 305ms/step - loss: 0.0996 - val_loss: 0.4559\n",
            "Epoch 11/20\n",
            "188/188 [==============================] - 57s 305ms/step - loss: 0.0980 - val_loss: 0.4560\n",
            "Epoch 12/20\n",
            "188/188 [==============================] - 57s 306ms/step - loss: 0.0959 - val_loss: 0.4566\n",
            "Epoch 13/20\n",
            "188/188 [==============================] - 58s 307ms/step - loss: 0.0942 - val_loss: 0.4604\n",
            "Epoch 14/20\n",
            "188/188 [==============================] - 57s 306ms/step - loss: 0.0929 - val_loss: 0.4614\n",
            "Epoch 15/20\n",
            "188/188 [==============================] - 58s 306ms/step - loss: 0.0909 - val_loss: 0.4633\n",
            "Epoch 16/20\n",
            "188/188 [==============================] - 57s 306ms/step - loss: 0.0895 - val_loss: 0.4640\n",
            "Epoch 17/20\n",
            "188/188 [==============================] - 57s 305ms/step - loss: 0.0879 - val_loss: 0.4664\n",
            "Epoch 18/20\n",
            "188/188 [==============================] - 57s 305ms/step - loss: 0.0865 - val_loss: 0.4682\n",
            "Epoch 19/20\n",
            "188/188 [==============================] - 57s 306ms/step - loss: 0.0850 - val_loss: 0.4694\n",
            "Epoch 20/20\n",
            "188/188 [==============================] - 57s 306ms/step - loss: 0.0833 - val_loss: 0.4704\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9fec4d54e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEH54ld7-vxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Encoder model for inference\n",
        "encoder_model=Model(input_encoder,encoder_lstm)\n",
        "# encoder_model takes a integer sequence of Italian language as input and returns hidden_state and cell_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYlXxX8F-vue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoder states - shape is LSTM_Size\n",
        "decoder_state_h=Input(shape=(256))\n",
        "decoder_state_c=Input(shape=(256))\n",
        "decoder_state_inputs=[decoder_state_h,decoder_state_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG9jSVMM-vr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input to decoder is send to embedding layer\n",
        "decoder_embedding_output=decoder_embedding_layer(input_decoder)\n",
        "\n",
        "# Get the decoder outputs when a particular input and initial_state is given\n",
        "decoder_outputs,state_h,state_c=decoder_lstm(decoder_embedding_output,initial_state=decoder_state_inputs)\n",
        "decoder_states=[state_h,state_c]\n",
        "# Send the decoder outputs to the dense layer to get the predicted word\n",
        "decoder_outputs=decoder_dense(decoder_outputs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4QLWJhtBlJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# decoder model takes two inputs 1. Decoder input 2. Decoder initial states\n",
        "# returns two outputs 1. Decoder outputs 2. decoder states(hidden_state and cell state)\n",
        "decoder_model=Model([input_decoder]+decoder_state_inputs,[decoder_outputs]+decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBwpfBz7BmkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "    \n",
        "x = PrettyTable()\n",
        "x.field_names = [\"Input data\", \"Actual data\", \"Predicted data\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnA-79FABrYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference(seq):\n",
        "  '''\n",
        "    Function that takes Italian sequence and returns English sequence\n",
        "  '''\n",
        "  # Input the sequence to encoder_model and get the final timestep encoder_states(Hidden and cell state)\n",
        "  seq=seq.reshape(-1,inp_seq_length)\n",
        "  encoder_state_value=encoder_model.predict(seq)\n",
        "  encoder_state_value=encoder_state_value[1:]\n",
        "  # encoder_state_value[0] -- hidden_state -- shape (None,256)\n",
        "  # encoder_state_value[1] -- cell_state -- shape (None,256)\n",
        "\n",
        "  # Target word\n",
        "  target_word=np.zeros((1,1))\n",
        "  # <start>:1 , <end>:2 -- \n",
        "  target_word[0,0]=1\n",
        "  \n",
        "  stop_condition=False\n",
        "  sent=''\n",
        "  k=0\n",
        "  while not stop_condition:\n",
        "\n",
        "      # We are giving a target_word which represents <start> and encoder_states to the decoder_model\n",
        "      output,state_h,state_c=decoder_model.predict([target_word]+encoder_state_value)\n",
        "      # As the target word length is 1. We will only have one time step\n",
        "      encoder_state_value=[state_h,state_c]\n",
        "      # Output shape (1,eng_vocab_size). Find the word which the decoder predicted with max_probability\n",
        "      output=np.argmax(output,-1)\n",
        "      #print(output)\n",
        "      # The output is a integer sequence, to get back the word. We use our lookup table reverse_dict\n",
        "      sent=sent+' '+ans_index_word[int(output)]\n",
        "      k+=1\n",
        "      # If the max_length of the sequence is reached or the model predicted 2 (<end>) stop your model\n",
        "      if k>tar_seq_length or output==2:\n",
        "        stop_condition=True\n",
        "      target_word=output.reshape(1,1)\n",
        "  \n",
        "  return sent\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMX56WnsB2t4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeXMqLSpBs3A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "7d73a432-3847-40ea-ada0-5aa083b5a23b"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "    \n",
        "x = PrettyTable()\n",
        "x.field_names = [\"Input data\", \"Actual data\", \"Predicted data\"]\n",
        "kk=[]\n",
        "for i in range(3):\n",
        "  index=random.randint(1,1000)\n",
        "  pred=inference(inp_lan[index])\n",
        "  kk.append(index)\n",
        "  x.add_row([data['Question'][index],data['Answer'][index],pred])\n",
        "  # print('Input data is: ',ita_data[index])\n",
        "  # print('Actual data is: ',eng_out[index])\n",
        "  # print('Predicted data is:',pred)\n",
        "print(x)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------+------------------------------------+----------------------------------+\n",
            "|                         Input data                        |            Actual data             |          Predicted data          |\n",
            "+-----------------------------------------------------------+------------------------------------+----------------------------------+\n",
            "|             Which president was least guilty?             | Lincoln, because he is in a cent.  |  because he is in a cent . <end> |\n",
            "| Whats the name of a bodybuilder whose a fan of the X-Men? |            Huge Jackman            |        huge they re <end>        |\n",
            "|  Why does Willem Dafoe play a villain in a lot of movies? |      Duh.  Cause he's da foe.      |              . <end>             |\n",
            "+-----------------------------------------------------------+------------------------------------+----------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HXMmR2WCuWw",
        "colab_type": "code",
        "outputId": "36557589-e76e-4f2c-ddac-9f60e1eb9452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "inp_lan[10]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   8,   31,   96,   52,   39,  671, 2525,   13,  241, 3907,    1,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyZAX6TDBunL",
        "colab_type": "code",
        "outputId": "ef557765-7ec9-4a88-c05e-91c567f1845f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data['Question'][10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why are there so many blood cells in female prisons?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKMvZoQqCT5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}